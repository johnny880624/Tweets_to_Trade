{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTrZr_Zfhh3S"
      },
      "source": [
        "## TEXT NORMALIZATION FUNCTION\n",
        "\n",
        "Below, we'll define a number of functions that perform various text prep-processing jobs, such as tokenization, lemmatization, etc. <br> After each function is defined, you can test that function by running it on a test sentence (object called test_text). <br> At the end of the notebook, we combine all those function into one function called normalize_corpus.<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyxjYS9_hh3V"
      },
      "source": [
        "We'll use the sentence below for testing the functions. It has punctuations signs, numbers, HTML markups, and other things to take care of:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bsp5JsZMhh3W"
      },
      "outputs": [],
      "source": [
        "test_text = \"<p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0QdSibohh3f"
      },
      "source": [
        "First, let's get Python modules ready and available (remember, you can always look up the modules in Python documentation to learn about what they do in detail):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "U_851f4ahh3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09f594da-2816-4c6a-d781-b42fa3b7d43b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting html.parser\n",
            "  Downloading html-parser-0.2.tar.gz (904 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ply\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: html.parser\n",
            "  Building wheel for html.parser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html.parser: filename=html_parser-0.2-py3-none-any.whl size=1320 sha256=72c28c6b45b8d7b6368a55ad793d2f1bbc423a4eade2f8be46edbb3218289fc9\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/59/31/4b5bc05833cb4b43b8359b80d98dd96ecf6d6e3e6edd0713df\n",
            "Successfully built html.parser\n",
            "Installing collected packages: ply, html.parser\n",
            "Successfully installed html.parser-0.2 ply-3.11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pattern3\n",
            "  Downloading pattern3-3.0.0.tar.gz (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from pattern3) (4.11.2)\n",
            "Collecting cherrypy\n",
            "  Downloading CherryPy-18.8.0-py2.py3-none-any.whl (348 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.4/348.4 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docx\n",
            "  Downloading docx-0.2.4.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer3k\n",
            "  Downloading pdfminer3k-1.3.4-py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting simplejson\n",
            "  Downloading simplejson-3.19.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.4/137.4 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m121.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->pattern3) (2.4.1)\n",
            "Collecting zc.lockfile\n",
            "  Downloading zc.lockfile-3.0.post1-py3-none-any.whl (9.8 kB)\n",
            "Collecting portend>=2.1.1\n",
            "  Downloading portend-3.1.0-py3-none-any.whl (5.3 kB)\n",
            "Collecting jaraco.collections\n",
            "  Downloading jaraco.collections-4.1.0-py3-none-any.whl (11 kB)\n",
            "Collecting cheroot>=8.2.1\n",
            "  Downloading cheroot-9.0.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.6/100.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.9/dist-packages (from cherrypy->pattern3) (9.1.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from docx->pattern3) (4.9.2)\n",
            "Requirement already satisfied: Pillow>=2.0 in /usr/local/lib/python3.9/dist-packages (from docx->pattern3) (8.4.0)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.9/dist-packages (from pdfminer.six->pattern3) (40.0.2)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from pdfminer.six->pattern3) (2.0.12)\n",
            "Requirement already satisfied: ply in /usr/local/lib/python3.9/dist-packages (from pdfminer3k->pattern3) (3.11)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from cheroot>=8.2.1->cherrypy->pattern3) (1.16.0)\n",
            "Collecting jaraco.functools\n",
            "  Downloading jaraco.functools-3.6.0-py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.9/dist-packages (from cryptography>=36.0.0->pdfminer.six->pattern3) (1.15.1)\n",
            "Collecting tempora>=1.8\n",
            "  Downloading tempora-5.2.2-py3-none-any.whl (13 kB)\n",
            "Collecting jaraco.text\n",
            "  Downloading jaraco.text-3.11.1-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from zc.lockfile->cherrypy->pattern3) (67.7.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern3) (2.21)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.9/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern3) (2022.7.1)\n",
            "Collecting jaraco.context>=4.1\n",
            "  Downloading jaraco.context-4.3.0-py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.9/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern3) (6.0.4)\n",
            "Collecting autocommand\n",
            "  Downloading autocommand-2.2.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from inflect->jaraco.text->jaraco.collections->cherrypy->pattern3) (1.10.7)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern3) (4.5.0)\n",
            "Building wheels for collected packages: pattern3, docx, sgmllib3k\n",
            "  Building wheel for pattern3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pattern3: filename=pattern3-3.0.0-py2.py3-none-any.whl size=18554353 sha256=c375fee31032367e4686843a72270c846906ddd649de9593154210b2628418b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/a6/70/271c00369ad669bd987786512b1ae28059c11a279a01405b10\n",
            "  Building wheel for docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx: filename=docx-0.2.4-py3-none-any.whl size=53922 sha256=93eb65733146581316b740822b1eee99bf968ad04996b019d558cddae1a9655a\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/8e/9d/7003eed35a84cf960876aae6bdf60d02041ddfcca66eceee94\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=ba0ea956239bb583b5be877f741f611b9a4c4d0889a5ec2a0a4cb6949057d146\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/7a/a7/78c287f64e401255dff4c13fdbc672fed5efbfd21c530114e1\n",
            "Successfully built pattern3 docx sgmllib3k\n",
            "Installing collected packages: sgmllib3k, zc.lockfile, simplejson, pdfminer3k, jaraco.functools, jaraco.context, feedparser, docx, autocommand, tempora, cheroot, portend, pdfminer.six, jaraco.text, jaraco.collections, cherrypy, pattern3\n",
            "Successfully installed autocommand-2.2.2 cheroot-9.0.0 cherrypy-18.8.0 docx-0.2.4 feedparser-6.0.10 jaraco.collections-4.1.0 jaraco.context-4.3.0 jaraco.functools-3.6.0 jaraco.text-3.11.1 pattern3-3.0.0 pdfminer.six-20221105 pdfminer3k-1.3.4 portend-3.1.0 sgmllib3k-1.0.0 simplejson-3.19.1 tempora-5.2.2 zc.lockfile-3.0.post1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (1.2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (1.2.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (3.1.2)\n",
            "Collecting pandas>=2.0.0\n",
            "  Downloading pandas-2.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numexpr in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (2.8.4)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (4.3.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (67.7.1)\n",
            "Collecting funcy\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (1.10.1)\n",
            "Collecting numpy>=1.24.2\n",
            "  Downloading numpy-1.24.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=2.0.0->pyLDAvis) (2022.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.1.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim->pyLDAvis) (6.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->pyLDAvis) (2.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Installing collected packages: funcy, numpy, pandas, pyLDAvis\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.3 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.5.3, but you have pandas 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed funcy-2.0 numpy-1.24.3 pandas-2.0.1 pyLDAvis-3.4.1\n"
          ]
        }
      ],
      "source": [
        "#module for supressing warnings about future changes in Python:\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "#module sys helps install modules from inside Jupyter:\n",
        "import sys\n",
        "\n",
        "#html parser for digesting text taken from web-pages\n",
        "!{sys.executable} -m pip install html.parser\n",
        "import html.parser\n",
        "from html.parser import HTMLParser\n",
        "\n",
        "#module for handling regular expressions and special characters\n",
        "import re\n",
        "\n",
        "#Natural Language ToolKit\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "#modules for handling unicode characters and strings\n",
        "import unicodedata\n",
        "import string\n",
        "!{sys.executable} -m pip install pattern3\n",
        "import pattern3\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import  CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "#pretty printing\n",
        "from pprint import pprint\n",
        "\n",
        "# Plotting tools\n",
        "!{sys.executable} -m pip install pyLDAvis\n",
        "import pyLDAvis\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZD_F1xkhh3g"
      },
      "source": [
        "Define functions for lemmatization and HTML parsing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_Lu1J3PVhh3g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb6e5ccd-822f-4f71-a7ce-1cdec6e8c988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "wnl = WordNetLemmatizer()\n",
        "html_parser = HTMLParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLBkhkR1hh3g"
      },
      "source": [
        "Defining the list of word contractions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ONeixIjehh3g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1ec70b6-cabd-45bd-e7a4-9457f00f4cf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "contraction_mapping = {\n",
        "\"ain't\": \"is not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"I'd\": \"I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I will\",\n",
        "\"I'll've\": \"I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UsNGiPshh3h"
      },
      "source": [
        "Select the list of stopwords from NLTK and amend it by adding more stopwords to it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GnMb7YGmhh3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a4fd053-ea94-48d3-ae30-bf1d0a673db0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "stopword_list = stopword_list + ['mr', 'mrs', 'come', 'go', 'get',\n",
        "                                 'tell', 'listen', 'one', 'two', 'three',\n",
        "                                 'four', 'five', 'six', 'seven', 'eight',\n",
        "                                 'nine', 'zero', 'join', 'find', 'make',\n",
        "                                 'say', 'ask', 'tell', 'see', 'try', 'back',\n",
        "                                 'also','would']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5GMhNyOhh3i"
      },
      "source": [
        "Split text into word tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EgZQd2Mqhh3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e80a59c6-6e1a-4458-c018-c85e96241c37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:  ['<', 'p', '>', 'The', 'circus', 'dog', 'in', 'a', 'plissé', 'skirt', 'jumped', 'over', 'Python', 'who', 'was', \"n't\", 'that', 'large', ',', 'just', '3', 'feet', 'long.', '<', '/p', '>']\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "def tokenize_text(text):\n",
        "    tokens = nltk.word_tokenize(text) \n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "print(\"Original:  \", test_text)\n",
        "print(\"Processed: \", tokenize_text(test_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLzMCA6Xhh3i"
      },
      "source": [
        "Expand contractions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lySxONKZhh3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca37910e-7a4a-450e-c187-1d18a563963d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:  <p>The circus dog in a plissé skirt jumped over Python who was not that large, just 3 feet long.</p>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def expand_contractions(text, contraction_mapping):\n",
        "    \n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
        "                                      flags=re.IGNORECASE|re.DOTALL)\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        first_char = match[0]\n",
        "        expanded_contraction = contraction_mapping.get(match)\\\n",
        "                                if contraction_mapping.get(match)\\\n",
        "                                else contraction_mapping.get(match.lower())                       \n",
        "        expanded_contraction = first_char+expanded_contraction[1:]\n",
        "        return expanded_contraction\n",
        "        \n",
        "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    return expanded_text\n",
        " \n",
        "print(\"Original:  \", test_text)\n",
        "print(\"Processed: \", expand_contractions(test_text,contraction_mapping))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFT5FmSahh3i"
      },
      "source": [
        "Annotate text tokens with Part-Of-Speach tags:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xunDWRiThh3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fcb4a16-2f66-4b0d-8688-599dfd8c0909"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:  [('<', 'a'), ('p', 'n'), ('>', 'v'), ('the', None), ('circus', 'n'), ('dog', 'n'), ('in', None), ('a', None), ('plissé', 'n'), ('skirt', 'n'), ('jumped', 'v'), ('over', None), ('python', 'n'), ('who', None), ('was', 'v'), (\"n't\", 'r'), ('that', None), ('large', 'a'), (',', None), ('just', 'r'), ('3', None), ('feet', 'n'), ('long.', 'a'), ('<', 'n'), ('/p', 'n'), ('>', 'n')]\n"
          ]
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def pos_tag_text(text_tokens):\n",
        "    def penn_to_wn_tags(pos_tag):\n",
        "        if pos_tag.startswith('J'):\n",
        "            return wn.ADJ\n",
        "        elif pos_tag.startswith('V'):\n",
        "            return wn.VERB\n",
        "        elif pos_tag.startswith('N'):\n",
        "            return wn.NOUN\n",
        "        elif pos_tag.startswith('R'):\n",
        "            return wn.ADV\n",
        "        else:\n",
        "            return None  \n",
        "    tagged_text = nltk.pos_tag(text_tokens)\n",
        "    tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag))\n",
        "                         for word, pos_tag in\n",
        "                         tagged_text]\n",
        "    return tagged_lower_text\n",
        "\n",
        "\n",
        "print(\"Original:  \", test_text)\n",
        "print(\"Processed: \", pos_tag_text(tokenize_text(test_text)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUfxX191hh3j"
      },
      "source": [
        "Lemmatize text based on Part-Of-Speach (POS) tags: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "k59Odkq1hh3j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ef167ef-0803-4552-ed50-376492018771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:  < p > the circus dog in a plissé skirt jump over python who be n't that large , just 3 foot long. < /p >\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def lemmatize_text(text):\n",
        "    pos_tagged_text = pos_tag_text(text)\n",
        "    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag\n",
        "                         else word                     \n",
        "                         for word, pos_tag in pos_tagged_text]\n",
        "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
        "    return lemmatized_text\n",
        " \n",
        "print(\"Original:  \", test_text)\n",
        "print(\"Processed: \", lemmatize_text(tokenize_text(test_text)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCPLMObHhh3j"
      },
      "source": [
        "Remove special characters, such as punctuation marks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BU8X8vMIhh3j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba7b4a87-0234-4f5d-ccb4-15fc66123f1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:    p   The circus dog in a plissé skirt jumped over Python who was n t that large   just 3 feet long     p  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def remove_special_characters(text):\n",
        "    tokens = tokenize_text(text)\n",
        "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
        "    filtered_tokens = filter(None, [pattern.sub(' ', token) for token in tokens])\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text \n",
        "\n",
        "print(\"Original:  \", test_text)\n",
        "print(\"Processed: \", remove_special_characters(test_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3an46pCyhh3k"
      },
      "source": [
        "Get rid of stopwords:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Q547ut1Dhh3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b28c9835-3ed9-4b36-89ec-9fcda0f563ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:  < p > The circus dog plissé skirt jumped Python n't large , 3 feet long. < /p >\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def remove_stopwords(text):\n",
        "    tokens = tokenize_text(text)\n",
        "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "\n",
        "print(\"Original:  \", test_text)\n",
        "print(\"Processed: \", remove_stopwords(test_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUHY0uQFhh3k"
      },
      "source": [
        "Remove all non-text characters (numbers, etc.):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LzLPzirAhh3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b025d8d-98e3-4c77-a8c5-c08c99b3a051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:  p The circus dog in a plissé skirt jumped over Python who was n't that large just feet long. /p\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def keep_text_characters(text):\n",
        "    filtered_tokens = []\n",
        "    tokens = tokenize_text(text)\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text\n",
        "\n",
        "print(\"Original:  \", test_text)\n",
        "print(\"Processed: \", keep_text_characters(test_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPoEvkoZhh3k"
      },
      "source": [
        "Clean up HTML markups: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "H2-FQRBLhh3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc5a33dd-419d-44e8-a6f8-2f2f4ef35a52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:  The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "class MLStripper(HTMLParser):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.reset()\n",
        "        self.fed = []\n",
        "    def handle_data(self, d):\n",
        "        self.fed.append(d)\n",
        "    def get_data(self):\n",
        "        return ' '.join(self.fed)\n",
        "    \n",
        "def strip_html(text):\n",
        "    html_stripper = MLStripper()\n",
        "    html_stripper.feed(text)\n",
        "    return html_stripper.get_data()\n",
        "\n",
        "print(\"Original:  \", test_text)\n",
        "print(\"Processed: \", strip_html(test_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oK5YF6byhh3k"
      },
      "source": [
        "Removing accents from characters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tI18vT5vhh3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8738d3d1-d000-400b-aa85-3acc96617744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:  <p>The circus dog in a plisse skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def normalize_accented_characters(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8')\n",
        "    return text\n",
        "\n",
        "print(\"Original:  \", test_text)\n",
        "print(\"Processed: \", normalize_accented_characters(test_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s64ne-UZhh3l"
      },
      "source": [
        "Putting all functions together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "FIPmLZDghh3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b97c3182-ebd2-44de-cd86-78c06f2361be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def normalize_corpus(corpus, only_text_chars=True):\n",
        "    normalized_corpus = []  \n",
        "    for index, text in enumerate(corpus):\n",
        "        text = normalize_accented_characters(text)\n",
        "        text = html.unescape(text)\n",
        "        text = strip_html(text)\n",
        "        text = expand_contractions(text, contraction_mapping)\n",
        "        text = tokenize_text(text)\n",
        "        text = lemmatize_text(text)\n",
        "        text = remove_special_characters(text)\n",
        "        text = remove_stopwords(text)\n",
        "        if only_text_chars:\n",
        "            text = keep_text_characters(text)\n",
        "        #text = tokenize_text(text)\n",
        "        normalized_corpus.append(text)    \n",
        "    return normalized_corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN_irUsjhh3l"
      },
      "source": [
        "Create a small corpus consisting of 2 test sentences and testing the normalize_corpus function on it (uncomment the lines below first):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nfXN9dSYhh3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "553b2d04-d28a-4e8a-f4dd-ea4e79919062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "test_corpus = [test_text]\n",
        "test_corpus.append(test_text)\n",
        "\n",
        "#print(\"Original:  \", test_corpus,\"\\n\")\n",
        "#print(\"Processed: \", normalize_corpus(test_corpus))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}